# Code Context Auditor — Implementation Plan

## Approach

Build a standalone audit skill that reads code-ref annotations from the context system, reads the actual source files, compares them using a calibrated AI prompt, and produces a structured report. The audit prompt is the most critical artifact — it must distinguish meaningful drift from acceptable abstraction.

## Design: Architecture

```
/audit-code-context (skill invocation)
    │
    ▼
Audit Orchestrator
    │
    ├── Read all code-ref snippets from index.yaml
    │       (or get_context(areas=["code-ref"]))
    │
    ├── For each snippet:
    │   ├── Read source file (from `source` frontmatter)
    │   ├── Parse AST (public methods, imports, class members)
    │   ├── Build audit context (annotation + code summary)
    │   └── Run audit prompt → Finding or "consistent"
    │
    ├── Collect all findings
    │
    ├── Generate coverage report
    │   ├── Count annotated vs unannotated files
    │   └── Flag large unannotated files
    │
    └── Write outputs
        ├── docs/project/audit/code-context-audit.md
        ├── docs/project/audit/audit-results.json
        └── Console summary
```

## Design: The Audit Prompt

This is the heart of the system. The prompt operates per-annotation, receiving the annotation text and a structured summary of the code.

### Input format per annotation

```
## Annotation
ID: code-ref/core/output-poller
Source: teleclaude/output_polling.py:OutputPoller
Claims: "Captures real-time terminal output from tmux sessions.
Streams tmux pane content to listeners via polling.
This module is responsible for output capture ONLY. It does not
format, deliver, or persist output."

## Code Summary
Class: OutputPoller
Public methods:
  - start_polling(session_id: str) -> None
  - stop_polling(session_id: str) -> None
  - get_latest_output(session_id: str) -> str
  - format_for_telegram(text: str) -> str  ← NOTE: formatting method
Imports from:
  - teleclaude.tmux_manager
  - teleclaude.adapters.telegram.escape  ← NOTE: adapter import
Internal methods: 3 private helpers
Lines of code: 187
```

### The code summary

The code summary is NOT generated by AI — it's extracted by the auditor's Python code using AST analysis:

- Public method signatures (name, params, return type)
- Class attributes and their types
- Import statements (grouped by origin)
- Line count
- Inheritance chain

This structured extraction means the AI prompt receives reliable data, not its own interpretation of the code. The AI's job is to compare the annotation claims against these facts.

### Prompt design principles

1. **Facts in, judgment out.** The code summary is mechanical (AST). The annotation is quoted verbatim. The AI's only job is the comparison.

2. **Calibrated sensitivity.** The prompt includes examples of "this IS a finding" and "this is NOT a finding" to calibrate the threshold.

3. **Boundary statements get special attention.** Any "does NOT do X" in an annotation is a testable claim. The prompt explicitly checks these against the code summary.

4. **Severity is about misleadingness.** Not about code quality. A perfectly clean module with a misleading annotation is high severity. A messy module with an accurate annotation is consistent.

### Full audit prompt

```
You are a code context auditor. Your job is to compare @context annotations
against the actual code to find inconsistencies.

You will receive:
1. An annotation (what the code claims to be responsible for)
2. A structured code summary (what the code actually contains)

## Your task

Compare the annotation's claims against the code summary's facts.
Report any inconsistencies using the finding types below.

## Finding types

- **drift**: Code has significant public functionality the annotation doesn't mention.
  Example: Annotation says "handles session lifecycle" but code has a
  `send_telegram_message()` method — that's not session lifecycle.

- **boundary_violation**: Annotation explicitly says "does NOT do X" but the code
  does X. This is the most serious type.
  Example: "Does not handle persistence" but code imports sqlite3 and has save methods.

- **omission**: Annotation is incomplete about the code's major responsibilities.
  Less severe than drift — the annotation isn't wrong, just missing things.
  Example: Annotation mentions 2 of 5 major public method groups.

- **naming**: The module/class name doesn't match the annotation's described purpose.
  Example: Class called `MessageOps` but annotation says "manages footer delivery."

## What is NOT a finding

- Private/internal helper methods not mentioned (implementation detail).
- Utility imports (logging, typing, dataclasses) not mentioned.
- The annotation being a high-level summary rather than exhaustive.
- Implementation patterns (caching, retries) not mentioned if they serve
  the declared responsibility.

## The key test

"If an agent read only this annotation and then modified this code,
would the annotation guide them correctly?"

- Yes, even if the annotation is simplified → CONSISTENT
- No, the agent would be surprised or misled → FINDING

## Boundary statement test

If the annotation contains "does NOT", "ONLY", "not responsible for",
or similar boundary language, verify each claim against the code summary.
Each violated boundary is a boundary_violation finding.

## Output

If consistent:
{ "result": "consistent" }

If finding:
{
  "result": "finding",
  "type": "<drift|boundary_violation|omission|naming>",
  "claims": "<quoted relevant part of annotation>",
  "reality": "<specific code evidence: method names, imports>",
  "severity": "<high|medium|low>",
  "action": "<specific suggested fix>"
}

Multiple findings per annotation are allowed. Return a JSON array.
```

## Design: Coverage Analysis

Beyond consistency checking, the auditor also reports annotation coverage:

1. Walk all Python files in configured source directories.
2. For each file, check if any code-ref snippet references it.
3. Flag files with >100 LOC and no annotation as coverage gap candidates.
4. Report: total files, annotated files, coverage %, gap list.

This helps agents know where to focus annotation effort.

## Files to Create

| File                                    | Purpose                                                          |
| --------------------------------------- | ---------------------------------------------------------------- |
| `teleclaude/code_context_auditor.py`    | Audit orchestrator: reads snippets, summarizes code, runs prompt |
| `agents/commands/audit-code-context.md` | Skill definition for `/audit-code-context`                       |
| `docs/project/audit/` (directory)       | Output location for reports                                      |

## Files to Modify

| File                         | Change                                                        |
| ---------------------------- | ------------------------------------------------------------- |
| `teleclaude/code_context.py` | Add AST summary extraction (reuse across scraper and auditor) |

## Task Sequence

### Task 1: AST Code Summarizer

**Files**: `teleclaude/code_context.py` (extend) or `teleclaude/code_context_auditor.py`

Build a function that takes a source file path and produces a structured summary:

```python
@dataclass
class CodeSummary:
    element_type: str        # "class", "function", "module"
    name: str
    public_methods: list[MethodInfo]  # name, params, return_type
    imports: list[str]       # grouped by source
    class_attributes: list[str]
    line_count: int
    inheritance: list[str]   # base classes

@dataclass
class MethodInfo:
    name: str
    params: list[str]
    return_type: str | None
    is_async: bool

def summarize_code(source_path: Path, element_name: str) -> CodeSummary:
    """Extract structured code summary using AST analysis."""
```

This is pure Python — no AI involved. Mechanical extraction.

**Verify**: Unit tests with sample classes/functions.

### Task 2: Audit Orchestrator

**Files**: `teleclaude/code_context_auditor.py`

```python
@dataclass
class AuditFinding:
    snippet_id: str
    finding_type: str  # drift, omission, naming, boundary_violation, orphan
    claims: str
    reality: str
    severity: str      # high, medium, low
    action: str

@dataclass
class AuditReport:
    timestamp: str
    total_audited: int
    consistent: int
    findings: list[AuditFinding]
    coverage: CoverageReport

@dataclass
class CoverageReport:
    total_files: int
    annotated_files: int
    coverage_percent: float
    gap_candidates: list[str]  # Large unannotated files

async def run_audit(project_root: Path) -> AuditReport:
    """Run the full audit pipeline."""
    # 1. Load all code-ref snippets from index
    # 2. For each snippet, extract code summary
    # 3. Build audit context (annotation + summary)
    # 4. Run audit prompt
    # 5. Collect findings
    # 6. Generate coverage report
    # 7. Return structured report
```

The orchestrator is NOT an AI — it's Python code that prepares data and delegates the comparison judgment to an AI prompt.

**Verify**: Integration test with a known-drift annotation.

### Task 3: Report Generation

**Files**: `teleclaude/code_context_auditor.py`

```python
def write_report(report: AuditReport, project_root: Path) -> None:
    """Write audit report to docs/project/audit/."""
    # Write markdown report
    # Write JSON results
    # Print console summary
```

**Verify**: Reports render correctly, JSON is valid.

### Task 4: Skill Definition

**Files**: `agents/commands/audit-code-context.md`

The skill invokes the auditor orchestrator. It needs to:

1. Load the code-ref index.
2. Run the audit.
3. Present findings.
4. Optionally suggest creating todos for high-severity findings.

The skill itself is a prompt that guides the agent through invoking the Python auditor and interpreting results.

**Verify**: Skill can be invoked via `/audit-code-context`.

### Task 5: Prompt Calibration

Create a test fixture with known-good and known-bad annotation/code pairs:

- **Consistent**: Accurate annotation, clean code → no findings.
- **Drift**: Code has extra responsibilities → drift finding.
- **Boundary violation**: "Does NOT do X" but code does X → boundary_violation.
- **Omission**: Annotation covers 2 of 5 responsibilities → omission finding.
- **Acceptable abstraction**: High-level annotation, detailed code → no findings.

Run the audit prompt against these fixtures. Adjust prompt until it correctly classifies all cases.

**This is the most important task.** The prompt must be calibrated before the tool is useful.

### Task 6: Tests

1. **Unit tests** for code summarizer:
   - Extracts public methods from a class.
   - Identifies imports.
   - Handles async functions.
   - Handles module-level functions.

2. **Unit tests** for report generation:
   - Markdown output matches expected format.
   - JSON output is valid and complete.
   - Coverage calculation is correct.

3. **Integration test**: End-to-end audit with test fixtures (Task 5).

## Risks and Assumptions

| Risk                                             | Mitigation                                                  |
| ------------------------------------------------ | ----------------------------------------------------------- |
| AI prompt produces inconsistent severity ratings | Calibration fixtures (Task 5); iterative prompt refinement  |
| AI over-reports findings (too sensitive)         | Prompt includes explicit "NOT a finding" examples           |
| AI under-reports findings (too permissive)       | Boundary violation test specifically tests explicit claims  |
| Code summary extraction misses relevant details  | AST-based; covers public API, imports, attributes           |
| Audit takes too long on large codebases          | Each annotation is independent; can parallelize             |
| Low annotation coverage makes audit useless      | Coverage report highlights this; minimum threshold guidance |

## Dependencies

- `code-context-annotations` must be complete (scraper producing snippets).
- Seed annotations must exist (minimum 5 for meaningful testing).
- The AI prompt comparison could use any model — doesn't need to be the most expensive. Haiku/Sonnet-class is sufficient for structured comparison.
